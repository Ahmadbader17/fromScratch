{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "Backpropagation (short for \"backward propagation of errors\") is the algorithm that enables neural networks to learn. It’s used to update the weights in a network so that it gets better at making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Implementing a Simple Neural Network (Without Backpropagation Yet)\n",
    "\n",
    "Before we introduce backprop, let’s first build a basic neural network with forward propagation.\n",
    "\n",
    "This will help us see how data flows forward before we start calculating gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python Code: Forward Pass in a Simple Neural Network\n",
    "We'll implement a 2-layer neural network with one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output (Before Training):\n",
      "[[0.53892274]\n",
      " [0.55132394]\n",
      " [0.5510619 ]\n",
      " [0.56117033]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "# sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "# Derivative of sigmoid (needed later for backpropagation)\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Sample input (2 features)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input data\n",
    "y = np.array([[0], [1], [1], [0]])  # Expected output (XOR problem)\n",
    "\n",
    "# Initialize weights randomly\n",
    "np.random.seed(42)\n",
    "input_layer_neurons = 2\n",
    "hidden_layer_neurons = 2\n",
    "output_neurons = 1\n",
    "\n",
    "# Randomly initializing weights\n",
    "W1 = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons))\n",
    "W2 = np.random.uniform(size=(hidden_layer_neurons, output_neurons))\n",
    "\n",
    "# Forward Propagation\n",
    "hidden_layer_input = np.dot(X, W1)  # Input to hidden layer\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)  # Activation function\n",
    "\n",
    "output_layer_input = np.dot(hidden_layer_output, W2)  # Input to output layer\n",
    "output = sigmoid(output_layer_input)  # Activation function\n",
    "\n",
    "print(\"Predicted Output (Before Training):\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Loss Function & Understanding Gradient Descent\n",
    "\n",
    "Now that we’ve implemented forward propagation, it’s time to measure how wrong our neural network’s predictions are. This is where the loss function comes in.\n",
    "\n",
    "A loss function calculates the difference between the predicted output and the actual target. Our goal is to minimize this loss so that the neural network makes better predictions.\n",
    "\n",
    "For a binary classification task, the most commonly used loss function is the Mean Squared Error (MSE) or Binary Cross-Entropy (Log Loss).\n",
    "For now, we’ll use MSE, which is simple and works well for small networks.\n",
    "\n",
    "Gradient Descent is the algorithm that helps adjust the weights of our network in a way that reduces the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python Code: Adding Loss Calculation\n",
    "\n",
    "Now, let’s modify our forward propagation code to include loss calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output (Before Training):\n",
      "[[0.53892274]\n",
      " [0.55132394]\n",
      " [0.5510619 ]\n",
      " [0.56117033]]\n",
      "\n",
      "Loss (Before Training): 0.2520513692725072\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Mean Squared Error (MSE) Loss Function\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Sample input (2 features)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input data\n",
    "y = np.array([[0], [1], [1], [0]])  # Expected output (XOR problem)\n",
    "\n",
    "# Initialize weights randomly\n",
    "np.random.seed(42)\n",
    "W1 = np.random.uniform(size=(2, 2))  # Weights for input to hidden layer\n",
    "W2 = np.random.uniform(size=(2, 1))  # Weights for hidden to output layer\n",
    "\n",
    "# Forward Propagation\n",
    "hidden_layer_input = np.dot(X, W1)  # Input to hidden layer\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)  # Activation function\n",
    "\n",
    "output_layer_input = np.dot(hidden_layer_output, W2)  # Input to output layer\n",
    "output = sigmoid(output_layer_input)  # Activation function\n",
    "\n",
    "# Calculate loss\n",
    "loss = mse_loss(y, output)\n",
    "\n",
    "print(\"Predicted Output (Before Training):\")\n",
    "print(output)\n",
    "print(\"\\nLoss (Before Training):\", loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Understanding the Math Behind Backpropagation\n",
    "\n",
    "Backpropagation is an algorithm used to compute the gradients of the loss function with respect to the weights and biases of the network.\n",
    "It helps us determine how much each weight contributed to the error, so we can update them accordingly.\n",
    "\n",
    "It consists of two main steps:\n",
    "\n",
    "    1) Forward Pass → Compute predictions & loss.\n",
    "    \n",
    "    2) Backward Pass (Backpropagation) → Compute gradients & update weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.2520513692725072\n",
      "Epoch 1000, Loss: 0.1586043938154397\n",
      "Epoch 2000, Loss: 0.07477982865962356\n",
      "Epoch 3000, Loss: 0.04564895718379279\n",
      "Epoch 4000, Loss: 0.031960089934838794\n",
      "Epoch 5000, Loss: 0.024253118171967437\n",
      "Epoch 6000, Loss: 0.0193892707812152\n",
      "Epoch 7000, Loss: 0.016071864833796197\n",
      "Epoch 8000, Loss: 0.013679104505030588\n",
      "Epoch 9000, Loss: 0.011879164486818524\n",
      "\n",
      "Final Predicted Output After Training:\n",
      "[[0.05432736]\n",
      " [0.89823995]\n",
      " [0.89824011]\n",
      " [0.13514441]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Mean Squared Error (MSE) Loss Function\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Sample input (2 features)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input data\n",
    "y = np.array([[0], [1], [1], [0]])  # Expected output (XOR problem)\n",
    "\n",
    "# Initialize weights randomly\n",
    "np.random.seed(42)\n",
    "W1 = np.random.uniform(size=(2, 2))  # Weights for input to hidden layer\n",
    "W2 = np.random.uniform(size=(2, 1))  # Weights for hidden to output layer\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.5\n",
    "\n",
    "# Training loop\n",
    "epochs = 10000  # Number of iterations\n",
    "for epoch in range(epochs):\n",
    "    # **Forward Propagation**\n",
    "    hidden_layer_input = np.dot(X, W1)  # Input to hidden layer\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)  # Activation function\n",
    "\n",
    "    output_layer_input = np.dot(hidden_layer_output, W2)  # Input to output layer\n",
    "    output = sigmoid(output_layer_input)  # Activation function\n",
    "\n",
    "    # **Compute Loss**\n",
    "    loss = mse_loss(y, output)\n",
    "\n",
    "    # **Backward Propagation**\n",
    "    # Compute gradient for output layer\n",
    "    output_error = y - output\n",
    "    output_delta = output_error * sigmoid_derivative(output)\n",
    "\n",
    "    # Compute gradient for hidden layer\n",
    "    hidden_layer_error = output_delta.dot(W2.T)\n",
    "    hidden_layer_delta = hidden_layer_error * sigmoid_derivative(hidden_layer_output)\n",
    "\n",
    "    # **Update Weights**\n",
    "    W2 += hidden_layer_output.T.dot(output_delta) * learning_rate\n",
    "    W1 += X.T.dot(hidden_layer_delta) * learning_rate\n",
    "\n",
    "    # Print loss every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "print(\"\\nFinal Predicted Output After Training:\")\n",
    "print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
