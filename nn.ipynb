{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1ï¸âƒ£ What is a Neural Network?\n",
    "\n",
    "A neural network is a mathematical model inspired by the human brain. Just like our brain has neurons that process information, a neural network has artificial neurons (also called perceptrons) that learn patterns from data.\n",
    "\n",
    "#### ðŸ” Real-World Analogy: Learning to Recognize Cats\n",
    "\n",
    "Imagine youâ€™re teaching a child to recognize cats.\n",
    "\n",
    "You show different pictures and say, â€œThis is a catâ€ or â€œThis is not a cat.â€ \\\n",
    "The child remembers patterns like whiskers, ears, and fur. \\\n",
    "Next time, when shown a new image, they use these patterns to decide if it's a cat. \\\n",
    "A neural network does exactly thisâ€”except it learns the patterns mathematically. \n",
    "\n",
    "#### 2ï¸âƒ£ The Core Structure of a Neural Network\n",
    "\n",
    "A neural network consists of three main layers:\n",
    "\n",
    "1ï¸âƒ£ Input Layer â€“ Takes raw data (e.g., pixel values of an image). \\\n",
    "2ï¸âƒ£ Hidden Layers â€“ Extracts features & learns patterns. \\\n",
    "3ï¸âƒ£ Output Layer â€“ Produces predictions (e.g., â€œCatâ€ or â€œNot Catâ€).\n",
    "\n",
    "**ðŸ–¼ Visual Representation:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Input Layer     Hidden Layer       Output Layer\n",
    "      [ X1  X2  X3 ] â†’ [ H1  H2  H3 ] â†’     [  Y  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3ï¸âƒ£ The Building Blocks of a Neural Network\n",
    "\n",
    "#### ðŸ”¹ Neurons (Perceptrons)\n",
    "\n",
    "Each **neuron** takes input, processes it, and passes the result forward.\n",
    "\n",
    "ðŸ”¢ **Mathematical Representation:** Each neuron calculates:\n",
    "\n",
    "$$Z = W_1X_1 + W_2X_2 + ... + W_nX_n + B$$\n",
    "\n",
    "where:\n",
    "* $X_i$ are inputs (features)\n",
    "* $W_i$ are weights (importance of each input)\n",
    "* $B$ is bias (adjusts output shift)\n",
    "\n",
    "#### ðŸ”¹ Activation Functions (Adding Non-Linearity)\n",
    "\n",
    "The **activation function** determines whether a neuron **fires** or not. Without it, the network is just a linear model.\n",
    "\n",
    "ðŸ›  **Common Activation Functions:**\n",
    "\n",
    "âœ… **Sigmoid** (Good for probabilities):\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "âœ… **ReLU** (Most commonly used in deep networks):\n",
    "\n",
    "$$g(z) = max(0, z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Forward Propagation?\n",
    "\n",
    "Forward propagation is the process of passing inputs through the network, layer by layer, to get the final output.\n",
    "\n",
    "Example: Predicting House Prices ðŸ \n",
    "\n",
    "Imagine you want to predict the price of a house using:\n",
    "\n",
    "* Size (square feet)\n",
    "* Number of bedrooms\n",
    "* Location quality (1-10 scale)\n",
    "* Letâ€™s assume a simple neural network with:\n",
    "\n",
    "* 3 input neurons (one for each feature)\n",
    "* 1 hidden layer (3 neurons)\n",
    "* 1 output neuron (house price prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Mathematics of Forward Propagation\n",
    "\n",
    "Each neuron in the **hidden layer** applies this formula:\n",
    "\n",
    "$$Z = W_1X_1 + W_2X_2 + W_3X_3 + B$$\n",
    "\n",
    "Then, it applies an **activation function** (e.g., ReLU or sigmoid) to introduce non-linearity:\n",
    "\n",
    "$$A = g(Z)$$\n",
    "\n",
    "The **output neuron** repeats this process to compute the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Forward Propagation in Python (OOP-based)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted house price: -1401.69\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the ReLU activation function\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "# Define a simple neural network class\n",
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights and biases randomly\n",
    "        self.W1 = np.random.randn(hidden_size, input_size)  # Weights for input to hidden layer\n",
    "        self.b1 = np.zeros((hidden_size, 1))               # Bias for hidden layer\n",
    "        self.W2 = np.random.randn(output_size, hidden_size) # Weights for hidden to output layer\n",
    "        self.b2 = np.zeros((output_size, 1))               # Bias for output layer\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform forward propagation.\n",
    "        X: Input data (features)\n",
    "        \"\"\"\n",
    "        # Compute the hidden layer activations\n",
    "        Z1 = np.dot(self.W1, X) + self.b1  # Linear transformation\n",
    "        A1 = relu(Z1)                      # Apply activation function\n",
    "        \n",
    "        # Compute the output layer\n",
    "        Z2 = np.dot(self.W2, A1) + self.b2\n",
    "        return Z2  # Output value (raw prediction)\n",
    "\n",
    "# Example usage\n",
    "np.random.seed(42)\n",
    "nn = SimpleNeuralNetwork(input_size=3, hidden_size=3, output_size=1)\n",
    "\n",
    "# Sample input (Size, Bedrooms, Location Quality)\n",
    "X = np.array([[1200], [3], [8]])  # Input column vector\n",
    "\n",
    "# Perform forward propagation\n",
    "prediction = nn.forward(X)\n",
    "print(f\"Predicted house price: {prediction[0,0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Backpropagation?\n",
    "\n",
    "Backpropagation is the algorithm that allows a neural network to **adjust its weights and biases** based on the error in its predictions.\n",
    "\n",
    "#### ðŸ›  Step-by-Step Process\n",
    "\n",
    "1. **Compute the error** between the predicted output and the actual value.\n",
    "2. **Calculate gradients** to see how much each weight contributed to the error.\n",
    "3. **Update weights** using gradient descent to minimize future errors.\n",
    "\n",
    "#### Understanding Loss & Gradient Descent\n",
    "\n",
    "A neural network learns by minimizing a **loss function**, which measures the difference between predicted and actual values.\n",
    "\n",
    "#### ðŸ”¹ Common Loss Functions\n",
    "\n",
    "* **Mean Squared Error (MSE)** (for regression) \n",
    "$$L = \\frac{1}{n} \\sum (Y - \\hat{Y})^2$$\n",
    "\n",
    "* **Cross-Entropy Loss** (for classification) \n",
    "$$L = -\\sum Y \\log(\\hat{Y})$$\n",
    "\n",
    "The network **reduces this loss** using **gradient descent**, which updates weights in the direction that decreases the error.\n",
    "\n",
    "#### The Mathematics of Backpropagation\n",
    "\n",
    "For each weight $W$, we compute:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W} = \\text{how much the loss changes with respect to } W$$\n",
    "\n",
    "Then, we update $W$ using **gradient descent**:\n",
    "\n",
    "$$W = W - \\alpha \\cdot \\frac{\\partial L}{\\partial W}$$\n",
    "\n",
    "where $\\alpha$ is the **learning rate** (controls how much we adjust weights per step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.4811\n",
      "Epoch 100, Loss: 0.1614\n",
      "Epoch 200, Loss: 0.1077\n",
      "Epoch 300, Loss: 0.0603\n",
      "Epoch 400, Loss: 0.0293\n",
      "Epoch 500, Loss: 0.0139\n",
      "Epoch 600, Loss: 0.0072\n",
      "Epoch 700, Loss: 0.0046\n",
      "Epoch 800, Loss: 0.0033\n",
      "Epoch 900, Loss: 0.0023\n",
      "Predicted price category: 0.50\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define activation functions and their derivatives\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    return (Z > 0).astype(float)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def sigmoid_derivative(Z):\n",
    "    return sigmoid(Z) * (1 - sigmoid(Z))\n",
    "\n",
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        np.random.seed(42)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.W1 = np.random.randn(hidden_size, input_size)\n",
    "        self.b1 = np.zeros((hidden_size, 1))\n",
    "        self.W2 = np.random.randn(output_size, hidden_size)\n",
    "        self.b2 = np.zeros((output_size, 1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.Z1 = np.dot(self.W1, X) + self.b1\n",
    "        self.A1 = relu(self.Z1)\n",
    "        self.Z2 = np.dot(self.W2, self.A1) + self.b2\n",
    "        self.A2 = sigmoid(self.Z2)\n",
    "        return self.A2\n",
    "\n",
    "    def backward(self, X, Y):\n",
    "        m = X.shape[1]  # Number of samples\n",
    "        \n",
    "        # Compute gradients\n",
    "        dZ2 = self.A2 - Y  # Error at output layer\n",
    "        dW2 = (1/m) * np.dot(dZ2, self.A1.T)\n",
    "        db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "        dZ1 = np.dot(self.W2.T, dZ2) * relu_derivative(self.Z1)\n",
    "        dW1 = (1/m) * np.dot(dZ1, X.T)\n",
    "        db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "        # Update weights using gradient descent\n",
    "        self.W1 -= self.learning_rate * dW1\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "        self.W2 -= self.learning_rate * dW2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "\n",
    "    def train(self, X, Y, epochs=1000):\n",
    "        for i in range(epochs):\n",
    "            self.forward(X)\n",
    "            self.backward(X, Y)\n",
    "            if i % 100 == 0:\n",
    "                loss = np.mean((self.A2 - Y) ** 2)\n",
    "                print(f\"Epoch {i}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Example dataset (X: 3 features, Y: binary output)\n",
    "X = np.array([[1200, 3, 8], [850, 2, 5], [1500, 4, 9]]).T / 1000\n",
    "Y = np.array([[1, 0, 1]])  # Expected outputs (1 = expensive, 0 = cheap)\n",
    "\n",
    "# Train neural network\n",
    "nn = SimpleNeuralNetwork(input_size=3, hidden_size=3, output_size=1, learning_rate=0.1)\n",
    "nn.train(X, Y, epochs=1000)\n",
    "\n",
    "# Predict on new data\n",
    "test_X = np.array([[1000], [3], [7]]) / 1000\n",
    "prediction = nn.forward(test_X)\n",
    "print(f\"Predicted price category: {prediction[0,0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1ï¸âƒ£ Key Challenges in Training Deep Networks\n",
    "\n",
    "When training deep networks, we face several challenges:\n",
    "\n",
    "âŒ **Slow Learning** â€“ Training can take a long time if optimization is inefficient. \\\n",
    "âŒ **Vanishing/Exploding Gradients** â€“ Gradients become too small or too large in deep networks. \\\n",
    "âŒ **Overfitting** â€“ The network memorizes training data instead of generalizing to new data. \\\n",
    "âŒ **Poor Convergence** â€“ The model gets stuck in bad local minima and stops improving.\n",
    "\n",
    "We'll now explore **optimization techniques** and **regularization methods** to tackle these problems.\n",
    "\n",
    "#### 2ï¸âƒ£ Optimization: Improving Gradient Descent\n",
    "\n",
    "#### ðŸ”¹ **Gradient Descent Recap**\n",
    "\n",
    "Gradient descent updates weights using:\n",
    "\n",
    "$$W = W - \\alpha \\cdot \\frac{\\partial L}{\\partial W}$$\n",
    "\n",
    "where $\\alpha$ is the **learning rate**.\n",
    "\n",
    "However, **vanilla gradient descent** has problems like slow convergence. Let's look at better optimization techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### ðŸ”¹ **Advanced Optimizers**\n",
    "\n",
    "âœ… **Stochastic Gradient Descent (SGD)**\n",
    "* Instead of computing gradients over **all** training samples, it updates weights **after each batch**.\n",
    "* Improves speed but introduces noise.\n",
    "\n",
    "âœ… **Momentum-based Gradient Descent**\n",
    "* Adds a \"velocity\" term to the weight updates to smoothen training.\n",
    "$$v_t = \\beta v_{t-1} + (1 - \\beta) \\frac{\\partial L}{\\partial W}$$\n",
    "$$W = W - \\alpha v_t$$\n",
    "* Helps escape bad local minima.\n",
    "\n",
    "âœ… **RMSprop (Root Mean Square Propagation)**\n",
    "* Uses adaptive learning rates by normalizing gradients.\n",
    "* Good for non-stationary data (e.g., time-series).\n",
    "\n",
    "âœ… **Adam (Adaptive Moment Estimation)**\n",
    "* Combines **Momentum + RMSprop**\n",
    "* Most widely used optimizer for deep learning.\n",
    "\n",
    "#### 3ï¸âƒ£ Regularization: Preventing Overfitting\n",
    "\n",
    "If a neural network is too complex, it **memorizes** training data instead of generalizing. Regularization helps by **adding constraints** to the model.\n",
    "\n",
    "#### ðŸ”¹ **L1 & L2 Regularization (Weight Decay)**\n",
    "\n",
    "* **L1 (Lasso)**: Encourages sparsity (some weights become 0).\n",
    "* **L2 (Ridge)**: Shrinks large weights, preventing overfitting.\n",
    "\n",
    "$$L = \\frac{1}{n} \\sum (Y - \\hat{Y})^2 + \\lambda \\sum W^2$$\n",
    "\n",
    "where $\\lambda$ is the regularization strength.\n",
    "\n",
    "#### ðŸ”¹ **Dropout Regularization**\n",
    "\n",
    "* **Randomly drops** neurons during training to prevent reliance on specific neurons.\n",
    "* Example: If dropout = 0.5, each neuron has a **50% chance of being ignored** in each step.\n",
    "\n",
    "#### ðŸ”¹ **Batch Normalization**\n",
    "\n",
    "* Normalizes activations to **reduce internal covariate shift**.\n",
    "* Speeds up training and improves stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Implementing Optimizers & Regularization in Python (OOP-Based):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.2500\n",
      "Epoch 100, Loss: 0.2104\n",
      "Epoch 200, Loss: 0.2036\n",
      "Epoch 300, Loss: 0.2036\n",
      "Epoch 400, Loss: 0.2039\n",
      "Epoch 500, Loss: 0.2163\n",
      "Epoch 600, Loss: 0.1353\n",
      "Epoch 700, Loss: 0.2127\n",
      "Epoch 800, Loss: 0.1951\n",
      "Epoch 900, Loss: 0.2088\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define activation functions and derivatives\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    return (Z > 0).astype(float)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def sigmoid_derivative(Z):\n",
    "    return sigmoid(Z) * (1 - sigmoid(Z))\n",
    "\n",
    "class OptimizedNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01, l2_lambda=0.01, dropout_rate=0.2):\n",
    "        np.random.seed(42)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.W1 = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        self.b1 = np.zeros((hidden_size, 1))\n",
    "        self.W2 = np.random.randn(output_size, hidden_size) * 0.01\n",
    "        self.b2 = np.zeros((output_size, 1))\n",
    "\n",
    "        # Adam Optimizer parameters\n",
    "        self.vdW1, self.vdW2 = 0, 0\n",
    "        self.sdW1, self.sdW2 = 0, 0\n",
    "        self.beta1, self.beta2, self.epsilon = 0.9, 0.999, 1e-8\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        self.Z1 = np.dot(self.W1, X) + self.b1\n",
    "        self.A1 = relu(self.Z1)\n",
    "        \n",
    "        # Dropout Regularization\n",
    "        if training:\n",
    "            self.dropout_mask = (np.random.rand(*self.A1.shape) > self.dropout_rate) / (1 - self.dropout_rate)\n",
    "            self.A1 *= self.dropout_mask\n",
    "\n",
    "        self.Z2 = np.dot(self.W2, self.A1) + self.b2\n",
    "        self.A2 = sigmoid(self.Z2)\n",
    "        return self.A2\n",
    "\n",
    "    def backward(self, X, Y):\n",
    "        m = X.shape[1]\n",
    "\n",
    "        # Compute gradients\n",
    "        dZ2 = self.A2 - Y\n",
    "        dW2 = (1/m) * np.dot(dZ2, self.A1.T) + (self.l2_lambda/m) * self.W2  # L2 Regularization\n",
    "        db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "        dZ1 = np.dot(self.W2.T, dZ2) * relu_derivative(self.Z1)\n",
    "        dW1 = (1/m) * np.dot(dZ1, X.T) + (self.l2_lambda/m) * self.W1  # L2 Regularization\n",
    "        db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "        # Adam Optimizer updates\n",
    "        self.vdW1 = self.beta1 * self.vdW1 + (1 - self.beta1) * dW1\n",
    "        self.vdW2 = self.beta1 * self.vdW2 + (1 - self.beta1) * dW2\n",
    "\n",
    "        self.sdW1 = self.beta2 * self.sdW1 + (1 - self.beta2) * (dW1 ** 2)\n",
    "        self.sdW2 = self.beta2 * self.sdW2 + (1 - self.beta2) * (dW2 ** 2)\n",
    "\n",
    "        # Bias correction\n",
    "        vdW1_corr = self.vdW1 / (1 - self.beta1)\n",
    "        vdW2_corr = self.vdW2 / (1 - self.beta1)\n",
    "        sdW1_corr = self.sdW1 / (1 - self.beta2)\n",
    "        sdW2_corr = self.sdW2 / (1 - self.beta2)\n",
    "\n",
    "        # Update weights\n",
    "        self.W1 -= self.learning_rate * vdW1_corr / (np.sqrt(sdW1_corr) + self.epsilon)\n",
    "        self.W2 -= self.learning_rate * vdW2_corr / (np.sqrt(sdW2_corr) + self.epsilon)\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "\n",
    "    def train(self, X, Y, epochs=1000):\n",
    "        for i in range(epochs):\n",
    "            self.forward(X)\n",
    "            self.backward(X, Y)\n",
    "            if i % 100 == 0:\n",
    "                loss = np.mean((self.A2 - Y) ** 2)\n",
    "                print(f\"Epoch {i}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Train the improved model\n",
    "X = np.array([[1200, 3, 8], [850, 2, 5], [1500, 4, 9]]).T / 1000\n",
    "Y = np.array([[1, 0, 1]])\n",
    "\n",
    "nn = OptimizedNeuralNetwork(input_size=3, hidden_size=3, output_size=1, learning_rate=0.01)\n",
    "nn.train(X, Y, epochs=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
