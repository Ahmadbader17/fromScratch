{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "Backpropagation (short for \"backward propagation of errors\") is the algorithm that enables neural networks to learn. It’s used to update the weights in a network so that it gets better at making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Implementing a Simple Neural Network (Without Backpropagation Yet)\n",
    "\n",
    "Before we introduce backprop, let’s first build a basic neural network with forward propagation.\n",
    "\n",
    "This will help us see how data flows forward before we start calculating gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python Code: Forward Pass in a Simple Neural Network\n",
    "We'll implement a 2-layer neural network with one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output (Before Training):\n",
      "[[0.53892274]\n",
      " [0.55132394]\n",
      " [0.5510619 ]\n",
      " [0.56117033]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "# sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "# Derivative of sigmoid (needed later for backpropagation)\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Sample input (2 features)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input data\n",
    "y = np.array([[0], [1], [1], [0]])  # Expected output (XOR problem)\n",
    "\n",
    "# Initialize weights randomly\n",
    "np.random.seed(42)\n",
    "input_layer_neurons = 2\n",
    "hidden_layer_neurons = 2\n",
    "output_neurons = 1\n",
    "\n",
    "# Randomly initializing weights\n",
    "W1 = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons))\n",
    "W2 = np.random.uniform(size=(hidden_layer_neurons, output_neurons))\n",
    "\n",
    "# Forward Propagation\n",
    "hidden_layer_input = np.dot(X, W1)  # Input to hidden layer\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)  # Activation function\n",
    "\n",
    "output_layer_input = np.dot(hidden_layer_output, W2)  # Input to output layer\n",
    "output = sigmoid(output_layer_input)  # Activation function\n",
    "\n",
    "print(\"Predicted Output (Before Training):\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Loss Function & Understanding Gradient Descent\n",
    "\n",
    "Now that we’ve implemented forward propagation, it’s time to measure how wrong our neural network’s predictions are. This is where the loss function comes in.\n",
    "\n",
    "A loss function calculates the difference between the predicted output and the actual target. Our goal is to minimize this loss so that the neural network makes better predictions.\n",
    "\n",
    "For a binary classification task, the most commonly used loss function is the Mean Squared Error (MSE) or Binary Cross-Entropy (Log Loss).\n",
    "For now, we’ll use MSE, which is simple and works well for small networks.\n",
    "\n",
    "Gradient Descent is the algorithm that helps adjust the weights of our network in a way that reduces the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python Code: Adding Loss Calculation\n",
    "\n",
    "Now, let’s modify our forward propagation code to include loss calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output (Before Training):\n",
      "[[0.53892274]\n",
      " [0.55132394]\n",
      " [0.5510619 ]\n",
      " [0.56117033]]\n",
      "\n",
      "Loss (Before Training): 0.2520513692725072\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Mean Squared Error (MSE) Loss Function\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Sample input (2 features)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input data\n",
    "y = np.array([[0], [1], [1], [0]])  # Expected output (XOR problem)\n",
    "\n",
    "# Initialize weights randomly\n",
    "np.random.seed(42)\n",
    "W1 = np.random.uniform(size=(2, 2))  # Weights for input to hidden layer\n",
    "W2 = np.random.uniform(size=(2, 1))  # Weights for hidden to output layer\n",
    "\n",
    "# Forward Propagation\n",
    "hidden_layer_input = np.dot(X, W1)  # Input to hidden layer\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)  # Activation function\n",
    "\n",
    "output_layer_input = np.dot(hidden_layer_output, W2)  # Input to output layer\n",
    "output = sigmoid(output_layer_input)  # Activation function\n",
    "\n",
    "# Calculate loss\n",
    "loss = mse_loss(y, output)\n",
    "\n",
    "print(\"Predicted Output (Before Training):\")\n",
    "print(output)\n",
    "print(\"\\nLoss (Before Training):\", loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Understanding the Math Behind Backpropagation\n",
    "\n",
    "Backpropagation is an algorithm used to compute the gradients of the loss function with respect to the weights and biases of the network.\n",
    "It helps us determine how much each weight contributed to the error, so we can update them accordingly.\n",
    "\n",
    "It consists of two main steps:\n",
    "\n",
    "    1) Forward Pass → Compute predictions & loss.\n",
    "    \n",
    "    2) Backward Pass (Backpropagation) → Compute gradients & update weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.2520513692725072\n",
      "Epoch 1000, Loss: 0.24998152415102137\n",
      "Epoch 2000, Loss: 0.24993098044436718\n",
      "Epoch 3000, Loss: 0.2499055627179052\n",
      "Epoch 4000, Loss: 0.2498778040725502\n",
      "Epoch 5000, Loss: 0.24984627319395852\n",
      "Epoch 6000, Loss: 0.24980969847108805\n",
      "Epoch 7000, Loss: 0.24976658130666474\n",
      "Epoch 8000, Loss: 0.24971512007938584\n",
      "Epoch 9000, Loss: 0.24965312082888597\n",
      "Epoch 10000, Loss: 0.2495778854717237\n",
      "Epoch 11000, Loss: 0.24948607106936638\n",
      "Epoch 12000, Loss: 0.24937351247123185\n",
      "Epoch 13000, Loss: 0.2492349996731948\n",
      "Epoch 14000, Loss: 0.24906400110084947\n",
      "Epoch 15000, Loss: 0.2488523258508311\n",
      "Epoch 16000, Loss: 0.2485897235600782\n",
      "Epoch 17000, Loss: 0.24826343282966618\n",
      "Epoch 18000, Loss: 0.24785771163736825\n",
      "Epoch 19000, Loss: 0.24735341936799463\n",
      "Epoch 20000, Loss: 0.2467277704548673\n",
      "Epoch 21000, Loss: 0.245954436538602\n",
      "Epoch 22000, Loss: 0.24500421534747002\n",
      "Epoch 23000, Loss: 0.24384647103738688\n",
      "Epoch 24000, Loss: 0.2424514379628092\n",
      "Epoch 25000, Loss: 0.24079325070327962\n",
      "Epoch 26000, Loss: 0.238853268556525\n",
      "Epoch 27000, Loss: 0.2366230269064933\n",
      "Epoch 28000, Loss: 0.23410610362516382\n",
      "Epoch 29000, Loss: 0.23131838139054103\n",
      "Epoch 30000, Loss: 0.2282865478522007\n",
      "Epoch 31000, Loss: 0.22504508991760266\n",
      "Epoch 32000, Loss: 0.22163240456123437\n",
      "Epoch 33000, Loss: 0.21808686929643958\n",
      "Epoch 34000, Loss: 0.21444370281166103\n",
      "Epoch 35000, Loss: 0.21073318811185082\n",
      "Epoch 36000, Loss: 0.20698041909378873\n",
      "Epoch 37000, Loss: 0.2032062965250314\n",
      "Epoch 38000, Loss: 0.1994291447283454\n",
      "Epoch 39000, Loss: 0.19566614505681576\n",
      "Epoch 40000, Loss: 0.19193390652475656\n",
      "Epoch 41000, Loss: 0.18824796367644847\n",
      "Epoch 42000, Loss: 0.18462161478733768\n",
      "Epoch 43000, Loss: 0.18106488379401042\n",
      "Epoch 44000, Loss: 0.17758422459072795\n",
      "Epoch 45000, Loss: 0.174183043685543\n",
      "Epoch 46000, Loss: 0.17086265246486523\n",
      "Epoch 47000, Loss: 0.16762317289441106\n",
      "Epoch 48000, Loss: 0.16446413491470369\n",
      "Epoch 49000, Loss: 0.16138476009165975\n",
      "Epoch 50000, Loss: 0.1583840541443888\n",
      "Epoch 51000, Loss: 0.15546082832877728\n",
      "Epoch 52000, Loss: 0.1526137139958333\n",
      "Epoch 53000, Loss: 0.14984118769731608\n",
      "Epoch 54000, Loss: 0.1471416033510598\n",
      "Epoch 55000, Loss: 0.14451322475029707\n",
      "Epoch 56000, Loss: 0.141954254463982\n",
      "Epoch 57000, Loss: 0.13946285792586044\n",
      "Epoch 58000, Loss: 0.13703718291379424\n",
      "Epoch 59000, Loss: 0.13467537507675448\n",
      "Epoch 60000, Loss: 0.13237559020705847\n",
      "Epoch 61000, Loss: 0.13013600386787472\n",
      "Epoch 62000, Loss: 0.1279548188794384\n",
      "Epoch 63000, Loss: 0.12583027107245168\n",
      "Epoch 64000, Loss: 0.12376063363886963\n",
      "Epoch 65000, Loss: 0.1217442203471475\n",
      "Epoch 66000, Loss: 0.11977938783835958\n",
      "Epoch 67000, Loss: 0.1178645371789288\n",
      "Epoch 68000, Loss: 0.11599811481298503\n",
      "Epoch 69000, Loss: 0.1141786130309655\n",
      "Epoch 70000, Loss: 0.11240457004972162\n",
      "Epoch 71000, Loss: 0.11067456978206486\n",
      "Epoch 72000, Loss: 0.10898724135959495\n",
      "Epoch 73000, Loss: 0.10734125846115564\n",
      "Epoch 74000, Loss: 0.10573533848986771\n",
      "Epoch 75000, Loss: 0.10416824163398317\n",
      "Epoch 76000, Loss: 0.10263876984048093\n",
      "Epoch 77000, Loss: 0.1011457657251175\n",
      "Epoch 78000, Loss: 0.0996881114383532\n",
      "Epoch 79000, Loss: 0.09826472750303145\n",
      "Epoch 80000, Loss: 0.0968745716367551\n",
      "Epoch 81000, Loss: 0.09551663756948045\n",
      "Epoch 82000, Loss: 0.09418995386483318\n",
      "Epoch 83000, Loss: 0.09289358275198781\n",
      "Epoch 84000, Loss: 0.09162661897356282\n",
      "Epoch 85000, Loss: 0.09038818865384043\n",
      "Epoch 86000, Loss: 0.08917744819065994\n",
      "Epoch 87000, Loss: 0.08799358317355035\n",
      "Epoch 88000, Loss: 0.08683580733000992\n",
      "Epoch 89000, Loss: 0.08570336150129704\n",
      "Epoch 90000, Loss: 0.0845955126486543\n",
      "Epoch 91000, Loss: 0.08351155289051415\n",
      "Epoch 92000, Loss: 0.08245079857093632\n",
      "Epoch 93000, Loss: 0.08141258935928\n",
      "Epoch 94000, Loss: 0.08039628738091038\n",
      "Epoch 95000, Loss: 0.07940127637858016\n",
      "Epoch 96000, Loss: 0.07842696090399877\n",
      "Epoch 97000, Loss: 0.07747276553899374\n",
      "Epoch 98000, Loss: 0.07653813414559837\n",
      "Epoch 99000, Loss: 0.07562252914432976\n",
      "\n",
      "Final Predicted Output After Training:\n",
      "[[0.20365675]\n",
      " [0.73606082]\n",
      " [0.73607457]\n",
      " [0.34366943]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Mean Squared Error (MSE) Loss Function\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Sample input (2 features)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input data\n",
    "y = np.array([[0], [1], [1], [0]])  # Expected output (XOR problem)\n",
    "\n",
    "# Initialize weights randomly\n",
    "np.random.seed(42)\n",
    "W1 = np.random.uniform(size=(2, 2))  # Weights for input to hidden layer\n",
    "W2 = np.random.uniform(size=(2, 1))  # Weights for hidden to output layer\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.5\n",
    "\n",
    "# Training loop\n",
    "epochs = 10000  # Number of iterations\n",
    "for epoch in range(epochs):\n",
    "    # **Forward Propagation**\n",
    "    hidden_layer_input = np.dot(X, W1)  # Input to hidden layer\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)  # Activation function\n",
    "\n",
    "    output_layer_input = np.dot(hidden_layer_output, W2)  # Input to output layer\n",
    "    output = sigmoid(output_layer_input)  # Activation function\n",
    "\n",
    "    # **Compute Loss**\n",
    "    loss = mse_loss(y, output)\n",
    "\n",
    "    # **Backward Propagation**\n",
    "    # Compute gradient for output layer\n",
    "    output_error = y - output\n",
    "    output_delta = output_error * sigmoid_derivative(output)\n",
    "\n",
    "    # Compute gradient for hidden layer\n",
    "    hidden_layer_error = output_delta.dot(W2.T)\n",
    "    hidden_layer_delta = hidden_layer_error * sigmoid_derivative(hidden_layer_output)\n",
    "\n",
    "    # **Update Weights**\n",
    "    W2 += hidden_layer_output.T.dot(output_delta) * learning_rate\n",
    "    W1 += X.T.dot(hidden_layer_delta) * learning_rate\n",
    "\n",
    "    # Print loss every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "print(\"\\nFinal Predicted Output After Training:\")\n",
    "print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
