{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stage 1: Foundational Concepts**\n",
    "\n",
    "#### **1.1 What is Logistic Regression?**\n",
    "\n",
    "Logistic Regression is a supervised learning algorithm used for **binary classification** tasks. It predicts the probability that an input belongs to one of two classes (e.g., \"Yes\" or \"No\", \"Spam\" or \"Not Spam\"). Despite its name, it is not used for regression but for classification.\n",
    "\n",
    "Key points:\n",
    "\n",
    "* Output is a probability value between 0 and 1.\n",
    "* The decision boundary is typically set at 0.5: if *P*(*y*=1∣*x*)≥0.5, predict class 1; otherwise, predict class 0.\n",
    "* It assumes a linear relationship between the input features and the log-odds of the output.\n",
    "\n",
    "#### **1.2 Why Use Logistic Regression?**\n",
    "   * **Simplicity**: Easy to implement and interpret.\n",
    "   * **Efficiency**: Fast to train, even on large datasets.\n",
    "   * **Probabilistic Output**: Provides probabilities, which can be useful for decision-making.\n",
    "   * **Baseline Model**: Often serves as a baseline for more complex models.\n",
    "\n",
    "\n",
    "#### **1.3 Key Terminology**\n",
    "\n",
    "Before diving deeper, let's define some key terms:\n",
    "\n",
    "   * **Binary Classification**: A task where the output has only two possible classes.\n",
    "   * **Logit Function**: The natural logarithm of the odds (log(p/(1−p))).\n",
    "   * **Sigmoid Function**: A mathematical function that maps any real-valued number into the range [0, 1].\n",
    "   * **Decision Boundary**: The threshold that separates the two classes.\n",
    "\n",
    "\n",
    "#### **1.4 The Sigmoid Function**\n",
    "\n",
    "The core of logistic regression is the **sigmoid function**, defined as:\n",
    "*σ*(*z*)=1/(1+*e*^(−*z*))\n",
    "\n",
    "Where:\n",
    "   * *z*=*w^T*·*x*+*b* is the linear combination of inputs *x*, weights *w*, and bias *b*.\n",
    "   * *σ*(*z*) outputs a value between 0 and 1, representing the probability of class 1.\n",
    "\n",
    "**Properties of the Sigmoid Function**:\n",
    "   * As *z*→∞, *σ*(*z*)→1.\n",
    "   * As *z*→−∞, *σ*(*z*)→0.\n",
    "   * At *z*=0, *σ*(*z*)=0.5.\n",
    "\n",
    "\n",
    "#### **1.5 Probabilistic Interpretation**\n",
    "\n",
    "Logistic Regression models the probability of class 1 as:\n",
    "*P*(*y*=1∣*x*)=*σ*(*w^T*·*x*+*b*)\n",
    "And the probability of class 0 as:\n",
    "*P*(*y*=0∣*x*)=1−*P*(*y*=1∣*x*)\n",
    "The model predicts the class with the highest probability.\n",
    "\n",
    "\n",
    "#### **Stage 2: Mathematical Derivation**\n",
    "\n",
    "#### **2.1 The Logistic Regression Model**\n",
    "\n",
    "Given a dataset {(*x*^(*i*),*y*^(*i*))}_{*i*=1}^{*n*}, where:\n",
    "   * *x*^(*i*)∈ℝ^*d* is the feature vector for the *i*-th sample.\n",
    "   * *y*^(*i*)∈{0,1} is the binary label.\n",
    "\n",
    "The logistic regression model predicts:\n",
    "\n",
    "*P*(*y*=1∣*x*)=*σ*(*w^T*·*x*+*b*)\n",
    "\n",
    "Where:\n",
    "   * *w*∈ℝ^*d* is the weight vector.\n",
    "   * *b*∈ℝ is the bias term.\n",
    "\n",
    "\n",
    "#### **2.2 Loss Function**\n",
    "\n",
    "To train the model, we need a loss function that measures how well the predicted probabilities match the true labels. Logistic Regression uses the **Binary \n",
    "\n",
    "**Cross-Entropy Loss**:\n",
    "\n",
    "$L(w,b) = -\\frac{1}{n}\\sum_{i=1}^{n}[y^{(i)}\\log(P(y=1|x^{(i)})) + (1-y^{(i)})\\log(1-P(y=1|x^{(i)}))]$\n",
    "\n",
    "Substituting $P(y=1|x^{(i)}) = \\sigma(w^Tx^{(i)}+b)$, the loss becomes:\n",
    "\n",
    "$L(w,b) = -\\frac{1}{n}\\sum_{i=1}^{n}[y^{(i)}\\log(\\sigma(w^Tx^{(i)}+b)) + (1-y^{(i)})\\log(1-\\sigma(w^Tx^{(i)}+b))]$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_j} = \\frac{1}{n}\\sum_{i=1}^{n}[\\sigma(w^Tx^{(i)}+b) - y^{(i)}]x_j^{(i)}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b} = \\frac{1}{n}\\sum_{i=1}^{n}[\\sigma(w^Tx^{(i)}+b) - y^{(i)}]$\n",
    "\n",
    "#### **2.3 Gradient Descent**\n",
    "\n",
    "\n",
    "Using these gradients, we update $w$ and $b$ iteratively:\n",
    "\n",
    "$w_j := w_j - \\alpha\\frac{\\partial L}{\\partial w_j}$\n",
    "\n",
    "$b := b - \\alpha\\frac{\\partial L}{\\partial b}$\n",
    "\n",
    "Where $\\alpha$ is the learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def initialize_parameters(dim):\n",
    "    w = np.zeros((dim, 1))\n",
    "    b = 0.0\n",
    "    return w, b\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    m = X.shape[1]\n",
    "\n",
    "    # Forward Propagation\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    cost = -1/m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n",
    "\n",
    "    # Backward Propagation\n",
    "    dw = 1/m * np.dot(X, (A - Y).T)\n",
    "    db = 1/m * np.sum(A - Y)\n",
    "\n",
    "    grads = {\"dw\": dw, \"db\": db}\n",
    "\n",
    "    return grads, cost\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate):\n",
    "    for i in range(num_iterations):\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Cost after iteration {i}: {cost}\")\n",
    "\n",
    "    params = {\"w\": w, \"b\": b}\n",
    "    return params\n",
    "\n",
    "def predict(w, b, X):\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "\n",
    "    for i in range(A.shape[1]):\n",
    "        Y_prediction[0, i] = 1 if A[0, i] > 0.5 else 0\n",
    "\n",
    "    return Y_prediction\n",
    "\n",
    "def model(X_train, Y_train, num_iterations=2000, learning_rate=0.5):\n",
    "    w, b = initialize_parameters(X_train.shape[0])\n",
    "    parameters = optimize(w, b, X_train, Y_train, num_iterations, learning_rate)\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let’s test the implementation on a simple dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.6931471805599453\n",
      "Cost after iteration 100: 2.638934186852725\n",
      "Cost after iteration 200: 1.0562398391277186\n",
      "Cost after iteration 300: 0.7594047322538188\n",
      "Cost after iteration 400: 0.8867141066305116\n",
      "Cost after iteration 500: 0.6531997430225687\n",
      "Cost after iteration 600: 1.024434540112927\n",
      "Cost after iteration 700: 1.0329198779365159\n",
      "Cost after iteration 800: 0.9518824441374012\n",
      "Cost after iteration 900: 2.569520318590931\n",
      "Cost after iteration 1000: 0.7184225918680391\n",
      "Cost after iteration 1100: 0.7255740571535026\n",
      "Cost after iteration 1200: 0.6646253753098406\n",
      "Cost after iteration 1300: 0.752144183333699\n",
      "Cost after iteration 1400: 2.9093590392223088\n",
      "Cost after iteration 1500: 1.1320000403016182\n",
      "Cost after iteration 1600: 2.60781689099062\n",
      "Cost after iteration 1700: 0.7410914097270515\n",
      "Cost after iteration 1800: 1.3161173113611984\n",
      "Cost after iteration 1900: 0.677039916993291\n",
      "Predictions: [[0. 0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example dataset\n",
    "    X_train = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "    Y_train = np.array([[0, 1, 0, 1]])\n",
    "\n",
    "    # Train the model\n",
    "    w, b = model(X_train.T, Y_train, num_iterations=2000, learning_rate=0.5)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = predict(w, b, X_train.T)\n",
    "    print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
