{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boost\n",
    "\n",
    "#### What is Gradient Boosting?\n",
    "\n",
    "Gradient Boosting is a machine learning ensemble technique that builds a strong predictive model by combining multiple weak models (typically decision trees). It works sequentially, where each new model corrects the errors of the previous one. The \"gradient\" in Gradient Boosting refers to the use of gradient descent to minimize errors.\n",
    "\n",
    "Key Terms:\n",
    "\n",
    "1. Ensemble Learning: Combining multiple models to improve performance.\n",
    "\n",
    "2. Weak Learner: A simple model that performs slightly better than random guessing (e.g., shallow decision trees).\n",
    "\n",
    "3. Residuals: The difference between the actual and predicted values (errors).\n",
    "\n",
    "4. Gradient Descent: An optimization algorithm used to minimize a loss function by iteratively moving toward the steepest descent.\n",
    "\n",
    "#### How Gradient Boosting Works:\n",
    "\n",
    "Start with an initial prediction (e.g., the mean of the target variable).\n",
    "\n",
    "Calculate the residuals (errors) of the current model.\n",
    "\n",
    "Train a new weak learner (e.g., a decision tree) to predict the residuals.\n",
    "\n",
    "Update the model by adding the predictions of the weak learner.\n",
    "\n",
    "Repeat steps 2–4 until a stopping condition is met (e.g., a fixed number of trees)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=3):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def _calculate_variance(self, y):\n",
    "        \"\"\"Calculate the variance of the target variable.\"\"\"\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        mean = sum(y) / len(y)\n",
    "        return sum((yi - mean) ** 2 for yi in y) / len(y)\n",
    "\n",
    "    def _split(self, X, y, feature_index, threshold):\n",
    "        \"\"\"Split the dataset based on a feature and threshold.\"\"\"\n",
    "        left_X, left_y, right_X, right_y = [], [], [], []\n",
    "        for i in range(len(X)):\n",
    "            if X[i][feature_index] <= threshold:\n",
    "                left_X.append(X[i])\n",
    "                left_y.append(y[i])\n",
    "            else:\n",
    "                right_X.append(X[i])\n",
    "                right_y.append(y[i])\n",
    "        return left_X, left_y, right_X, right_y\n",
    "\n",
    "    def _find_best_split(self, X, y):\n",
    "        \"\"\"Find the best feature and threshold to split the data.\"\"\"\n",
    "        best_feature, best_threshold, best_variance = None, None, float('inf')\n",
    "        for feature_index in range(len(X[0])):\n",
    "            thresholds = set([x[feature_index] for x in X])\n",
    "            for threshold in thresholds:\n",
    "                left_X, left_y, right_X, right_y = self._split(X, y, feature_index, threshold)\n",
    "                if len(left_y) == 0 or len(right_y) == 0:\n",
    "                    continue\n",
    "                total_variance = (len(left_y) * self._calculate_variance(left_y) +\n",
    "                                  len(right_y) * self._calculate_variance(right_y)) / len(y)\n",
    "                if total_variance < best_variance:\n",
    "                    best_feature, best_threshold, best_variance = feature_index, threshold, total_variance\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        \"\"\"Recursively build the decision tree.\"\"\"\n",
    "        if depth >= self.max_depth or len(y) <= 1:\n",
    "            return {'prediction': sum(y) / len(y)}\n",
    "        feature, threshold = self._find_best_split(X, y)\n",
    "        if feature is None:\n",
    "            return {'prediction': sum(y) / len(y)}\n",
    "        left_X, left_y, right_X, right_y = self._split(X, y, feature, threshold)\n",
    "        return {\n",
    "            'feature_index': feature,\n",
    "            'threshold': threshold,\n",
    "            'left': self._build_tree(left_X, left_y, depth + 1),\n",
    "            'right': self._build_tree(right_X, right_y, depth + 1)\n",
    "        }\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the decision tree to the data.\"\"\"\n",
    "        self.tree = self._build_tree(X, y, 0)\n",
    "\n",
    "    def _predict_sample(self, x, tree):\n",
    "        \"\"\"Predict a single sample using the decision tree.\"\"\"\n",
    "        if 'prediction' in tree:\n",
    "            return tree['prediction']\n",
    "        feature_index = tree['feature_index']\n",
    "        threshold = tree['threshold']\n",
    "        if x[feature_index] <= threshold:\n",
    "            return self._predict_sample(x, tree['left'])\n",
    "        else:\n",
    "            return self._predict_sample(x, tree['right'])\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the target for multiple samples.\"\"\"\n",
    "        return [self._predict_sample(x, self.tree) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, lr=0.01, max_depth=3):\n",
    "        self.estimators = n_estimators\n",
    "        self.lr = lr\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.initial_predictions = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit the gradient boosting model to the data.\"\"\"\n",
    "        # step 1: initialise the model with the mean of the targer variable \n",
    "        self.initial_predictions = sum(y) / len(y)\n",
    "        F = [self.initial_predictions] * len(y)\n",
    "\n",
    "        for _ in range(self.estimators):\n",
    "            # step 2: compute the residuals (negative gradient)\n",
    "            residuals = [y[i] - F[i] for i in range(len(y))]\n",
    "\n",
    "            # step 3: fit a decision tree to the residuals \n",
    "            tree = DecisionTree(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # step 4: update the model predictions \n",
    "            predictions = tree.predict(X)\n",
    "            for i in range(len(F)):\n",
    "                F[i] += self.lr * predictions[i]\n",
    "\n",
    "            # save the tree\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict the target for new data\"\"\"\n",
    "        # start with the initial prediction\n",
    "        predictions = [self.initial_predictions] * len(X)\n",
    "\n",
    "        # add predictions from all trees\n",
    "        for tree in self.trees:\n",
    "            tree_predictions = tree.predict(X)\n",
    "            for i in range(len(predictions)):\n",
    "                predictions[i] += self.lr * tree_predictions[i]\n",
    "\n",
    "        \n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (353, 10)\n",
      "Testing data shape: (89, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "data = load_diabetes()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Testing data shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Gradient Boosting model\n",
    "model = GradientBoostingRegressor(n_estimators=50, lr=0.1, max_depth=3)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 2858.724774704767\n",
      "R-squared (R²): 0.4604298045058516\n"
     ]
    }
   ],
   "source": [
    "# Calculate Mean Squared Error (MSE)\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return sum((y_true[i] - y_pred[i]) ** 2 for i in range(len(y_true))) / len(y_true)\n",
    "\n",
    "# Calculate R-squared (R²)\n",
    "def r_squared(y_true, y_pred):\n",
    "    mean_y = sum(y_true) / len(y_true)\n",
    "    ss_total = sum((y_true[i] - mean_y) ** 2 for i in range(len(y_true)))\n",
    "    ss_residual = sum((y_true[i] - y_pred[i]) ** 2 for i in range(len(y_true)))\n",
    "    return 1 - (ss_residual / ss_total)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r_squared(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"R-squared (R²):\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
