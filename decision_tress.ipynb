{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "What is Gini Impurity?\n",
    "Gini Impurity is a measure of how often a randomly chosen element from a set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. It ranges from 0 to 0.5:\n",
    "\n",
    "0: The set is perfectly pure (all elements belong to the same class).\n",
    "\n",
    "0.5: The set is maximally impure (elements are evenly distributed across classes).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [np.int64(1), np.int64(0), np.int64(2), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(0), np.int64(2), np.int64(0), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(0), np.int64(0)]\n",
      "True Labels: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Helper function to calculate Gini Impurity\n",
    "def gini_impurity(labels):\n",
    "    \"\"\"\n",
    "    Calculate the Gini Impurity for a set of labels.\n",
    "    Gini = 1 - sum(p_i^2), where p_i is the probability of class i.\n",
    "    \"\"\"\n",
    "    if len(labels) == 0:\n",
    "        return 0\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    probabilities = counts / len(labels)\n",
    "    return 1 - np.sum(probabilities ** 2)\n",
    "\n",
    "# Function to find the best split based on Gini Impurity\n",
    "def find_best_split(X, y):\n",
    "    \"\"\"\n",
    "    Find the best feature and split value to minimize Gini Impurity.\n",
    "    \"\"\"\n",
    "    best_gini = np.inf\n",
    "    best_feature = None\n",
    "    best_split_value = None\n",
    "\n",
    "    # Iterate over each feature\n",
    "    for feature_index in range(X.shape[1]):\n",
    "        feature_values = X[:, feature_index]\n",
    "        unique_values = np.unique(feature_values)\n",
    "\n",
    "        # Try splitting at each unique value of the feature\n",
    "        for value in unique_values:\n",
    "            left_mask = feature_values <= value\n",
    "            right_mask = feature_values > value\n",
    "            left_labels = y[left_mask]\n",
    "            right_labels = y[right_mask]\n",
    "\n",
    "            # Skip invalid splits\n",
    "            if len(left_labels) == 0 or len(right_labels) == 0:\n",
    "                continue\n",
    "\n",
    "            # Calculate weighted Gini Impurity\n",
    "            total_samples = len(y)\n",
    "            left_weight = len(left_labels) / total_samples\n",
    "            right_weight = len(right_labels) / total_samples\n",
    "            weighted_gini = left_weight * gini_impurity(left_labels) + right_weight * gini_impurity(right_labels)\n",
    "\n",
    "            # Update the best split if this one is better\n",
    "            if weighted_gini < best_gini:\n",
    "                best_gini = weighted_gini\n",
    "                best_feature = feature_index\n",
    "                best_split_value = value\n",
    "\n",
    "    return best_feature, best_split_value\n",
    "\n",
    "# Class for Decision Tree Node\n",
    "class TreeNode:\n",
    "    def __init__(self, feature=None, split_value=None, left=None, right=None, label=None):\n",
    "        self.feature = feature  # Feature index used for splitting\n",
    "        self.split_value = split_value  # Threshold value for splitting\n",
    "        self.left = left  # Left child node\n",
    "        self.right = right  # Right child node\n",
    "        self.label = label  # Label for leaf nodes\n",
    "\n",
    "# Recursive function to build the Decision Tree\n",
    "def build_tree(X, y, max_depth, current_depth=0):\n",
    "    \"\"\"\n",
    "    Recursively build the Decision Tree using Gini Impurity.\n",
    "    \"\"\"\n",
    "    # Base case: Stop if the tree reaches max depth or all labels are the same\n",
    "    if current_depth >= max_depth or len(np.unique(y)) == 1:\n",
    "        # Return a leaf node with the majority class label\n",
    "        unique_labels, counts = np.unique(y, return_counts=True)\n",
    "        majority_label = unique_labels[np.argmax(counts)]\n",
    "        return TreeNode(label=majority_label)\n",
    "\n",
    "    # Find the best split\n",
    "    best_feature, best_split_value = find_best_split(X, y)\n",
    "\n",
    "    # If no valid split is found, return a leaf node\n",
    "    if best_feature is None:\n",
    "        unique_labels, counts = np.unique(y, return_counts=True)\n",
    "        majority_label = unique_labels[np.argmax(counts)]\n",
    "        return TreeNode(label=majority_label)\n",
    "\n",
    "    # Split the dataset\n",
    "    left_mask = X[:, best_feature] <= best_split_value\n",
    "    right_mask = X[:, best_feature] > best_split_value\n",
    "    left_X, left_y = X[left_mask], y[left_mask]\n",
    "    right_X, right_y = X[right_mask], y[right_mask]\n",
    "\n",
    "    # Recursively build left and right subtrees\n",
    "    left_child = build_tree(left_X, left_y, max_depth, current_depth + 1)\n",
    "    right_child = build_tree(right_X, right_y, max_depth, current_depth + 1)\n",
    "\n",
    "    # Return a decision node\n",
    "    return TreeNode(feature=best_feature, split_value=best_split_value, left=left_child, right=right_child)\n",
    "\n",
    "# Function to predict using the Decision Tree\n",
    "def predict(tree, x):\n",
    "    \"\"\"\n",
    "    Traverse the Decision Tree to predict the class label for a single data point.\n",
    "    \"\"\"\n",
    "    if tree.label is not None:  # Leaf node\n",
    "        return tree.label\n",
    "    if x[tree.feature] <= tree.split_value:\n",
    "        return predict(tree.left, x)\n",
    "    else:\n",
    "        return predict(tree.right, x)\n",
    "\n",
    "# Wrapper function to train and use the Decision Tree\n",
    "def decision_tree_classifier(X_train, y_train, max_depth):\n",
    "    \"\"\"\n",
    "    Train a Decision Tree classifier and return the root node.\n",
    "    \"\"\"\n",
    "    return build_tree(X_train, y_train, max_depth)\n",
    "\n",
    "# Example Usage with Iris Dataset\n",
    "if __name__ == \"__main__\":\n",
    "    # Load Iris dataset\n",
    "    iris = load_iris()\n",
    "    X = iris.data  # Features: sepal length, sepal width, petal length, petal width\n",
    "    y = iris.target  # Labels: 0 (setosa), 1 (versicolor), 2 (virginica)\n",
    "\n",
    "    # Split into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train the Decision Tree\n",
    "    max_depth = 3\n",
    "    tree = decision_tree_classifier(X_train, y_train, max_depth)\n",
    "\n",
    "    # Test the Decision Tree\n",
    "    predictions = [predict(tree, x) for x in X_test]\n",
    "\n",
    "    # Evaluate accuracy\n",
    "    accuracy = np.mean(predictions == y_test)\n",
    "    print(\"Predictions:\", predictions)\n",
    "    print(\"True Labels:\", y_test)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
