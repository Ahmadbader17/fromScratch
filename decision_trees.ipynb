{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Foundations of Decision Trees\n",
    "\n",
    "A decision tree is a supervised machine learning algorithm used for classification and regression tasks. It models decisions in a tree-like structure, where:\n",
    "\n",
    "* Each node represents a feature or attribute.\n",
    "* Each branch represents a decision based on the feature.\n",
    "* Each leaf node represents the final outcome (class label or numerical prediction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Use Decision Trees?\n",
    "\n",
    "Decision trees are popular because:\n",
    "\n",
    "✅ They are interpretable (easy to understand and visualize). \\\n",
    "✅ They handle both categorical and numerical data. \\\n",
    "✅ They require little data preprocessing (no need for feature scaling). \\\n",
    "✅ They work well with nonlinear relationships. \\\n",
    "✅ They can be used for both classification and regression tasks. \n",
    "\n",
    "However, they have drawbacks:\n",
    "\n",
    "❌ Prone to overfitting (if too deep). \\\n",
    "❌ Sensitive to small changes in data. \\\n",
    "❌ Biased towards dominant features (if not properly tuned)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measuring Impurity in Decision Trees\n",
    "\n",
    "A decision tree aims to create pure groups (nodes where all samples belong to the same class). To measure impurity, we use three common metrics:\n",
    "\n",
    "1️⃣ Entropy (used in ID3 Algorithm) \\\n",
    "2️⃣ Gini Impurity (used in CART Algorithm) \\\n",
    "3️⃣ Mean Squared Error (MSE) (used in regression trees) \n",
    "\n",
    "Let’s break them down one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Impurity Measures & Information Gain\n",
    "\n",
    "#### 1️⃣ Entropy (Used in Classification Trees)\n",
    "\n",
    "Entropy quantifies the **uncertainty** (randomness) in a dataset.\n",
    "* A **pure** node (all samples belong to the same class) has **entropy = 0**.\n",
    "* A **mixed** node has **higher entropy** (more disorder).\n",
    "\n",
    "**Formula**:\n",
    "$$H(S) = - \\sum p_i \\log_2 p_i$$\n",
    "\n",
    "Where:\n",
    "* $p_i$ = proportion of samples in class $i$\n",
    "* Sum is over all possible classes\n",
    "\n",
    "#### 📌 Example Calculation\n",
    "If a dataset has:\n",
    "* **4 apples** 🍎\n",
    "* **6 oranges** 🍊\n",
    "\n",
    "The entropy is:\n",
    "$$H = -\\left(\\frac{4}{10} \\log_2 \\frac{4}{10} + \\frac{6}{10} \\log_2 \\frac{6}{10}\\right) \\approx 0.97$$\n",
    "\n",
    "🔵 **Lower entropy = purer node**  \n",
    "🔴 **Higher entropy = more mixed node**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 2️⃣ Gini Impurity (Used in CART Algorithm)\n",
    "\n",
    "Gini impurity is another measure of impurity. It represents the probability that a randomly chosen element is incorrectly classified if we label it randomly based on class proportions.\n",
    "\n",
    "**Formula**:\n",
    "$$\\text{Gini}(S) = 1 - \\sum p_i^2$$\n",
    "\n",
    "### 📌 Example Calculation\n",
    "For the same dataset:\n",
    "$$\\text{Gini} = 1 - \\left(\\left(\\frac{4}{10}\\right)^2 + \\left(\\frac{6}{10}\\right)^2\\right) = 1 - (0.16 + 0.36) = 0.48$$\n",
    "\n",
    "🔹 **Lower Gini = purer node**  \n",
    "🔹 **Higher Gini = more mixed node**\n",
    "\n",
    "👉 **Gini vs. Entropy**:\n",
    "* **Entropy** penalizes impurity more than Gini, so it is more sensitive to class proportions.\n",
    "* **Gini** is faster to compute since it avoids logarithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3️⃣ Mean Squared Error (Used in Regression Trees)\n",
    "\n",
    "For regression trees, we don't classify data but predict **continuous values**. Instead of entropy/Gini, we minimize **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n} \\sum (y_i - \\hat{y})^2$$\n",
    "\n",
    "Where:\n",
    "* $y_i$ = actual value\n",
    "* $\\hat{y}$ = predicted value\n",
    "* $n$ = number of samples\n",
    "\n",
    "🔹 **Lower MSE = better split**  \n",
    "🔹 **Higher MSE = poor split**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information Gain\n",
    "\n",
    "To decide which feature to split on, we use **Information Gain (IG)**. It measures how much impurity decreases when splitting a node.\n",
    "\n",
    "**Formula (for entropy-based trees)**:\n",
    "$$\\text{IG} = H(\\text{parent}) - \\sum \\left( \\frac{N_{\\text{child}}}{N_{\\text{parent}}} \\times H(\\text{child}) \\right)$$\n",
    "\n",
    "#### 📌 Example Calculation\n",
    "If we split our apple/orange dataset by color and get two subsets:\n",
    "* 🍎 **(4 apples, 0 oranges) → Entropy = 0**\n",
    "* 🍊 **(0 apples, 6 oranges) → Entropy = 0**\n",
    "\n",
    "The **Information Gain** is:\n",
    "$$\\text{IG} = 0.97 - (0.4 \\times 0 + 0.6 \\times 0) = 0.97$$\n",
    "\n",
    "✅ Since **entropy drops to 0**, this is a **perfect split**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Coding the Decision Tree from Scratch:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature        # Feature index to split on\n",
    "        self.threshold = threshold    # Value to split at (for numerical features)\n",
    "        self.left = left              # Left child node\n",
    "        self.right = right            # Right child node\n",
    "        self.value = value            # Class label (for leaf nodes)\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        \"\"\"Check if the node is a leaf node (no children).\"\"\"\n",
    "        return self.value is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=10, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Builds the decision tree.\"\"\"\n",
    "        self.root = self._grow_tree(X, y, depth=0)\n",
    "\n",
    "    def _grow_tree(self, X, y, depth):\n",
    "        \"\"\"Recursively splits the data and builds the tree.\"\"\"\n",
    "        num_samples, num_features = X.shape\n",
    "        unique_classes = np.unique(y)\n",
    "\n",
    "        # Stopping conditions\n",
    "        if (depth >= self.max_depth or len(unique_classes) == 1 or num_samples < self.min_samples_split):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        # Find the best split\n",
    "        best_feature, best_threshold = self._best_split(X, y, num_features)\n",
    "\n",
    "        # Split the data\n",
    "        left_indices = X[:, best_feature] <= best_threshold\n",
    "        right_indices = ~left_indices\n",
    "        left_child = self._grow_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_child = self._grow_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "        return Node(feature=best_feature, threshold=best_threshold, left=left_child, right=right_child)\n",
    "\n",
    "    def _best_split(self, X, y, num_features):\n",
    "        \"\"\"Finds the best feature and threshold for splitting the data.\"\"\"\n",
    "        best_gain = -1\n",
    "        best_feature, best_threshold = None, None\n",
    "\n",
    "        for feature in range(num_features):\n",
    "            thresholds = np.unique(X[:, feature])  # Possible split points\n",
    "            for threshold in thresholds:\n",
    "                gain = self._information_gain(y, X[:, feature], threshold)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _information_gain(self, y, feature_column, threshold):\n",
    "        \"\"\"Computes the information gain from a split.\"\"\"\n",
    "        parent_gini = self._gini(y)\n",
    "        left_indices = feature_column <= threshold\n",
    "        right_indices = ~left_indices\n",
    "\n",
    "        if len(y[left_indices]) == 0 or len(y[right_indices]) == 0:\n",
    "            return 0  # No split occurs\n",
    "\n",
    "        left_gini = self._gini(y[left_indices])\n",
    "        right_gini = self._gini(y[right_indices])\n",
    "        left_weight = len(y[left_indices]) / len(y)\n",
    "        right_weight = len(y[right_indices]) / len(y)\n",
    "\n",
    "        # Weighted average of child node impurities\n",
    "        gini_gain = parent_gini - (left_weight * left_gini + right_weight * right_gini)\n",
    "        return gini_gain\n",
    "\n",
    "    def _gini(self, y):\n",
    "        \"\"\"Computes Gini impurity of a node.\"\"\"\n",
    "        class_counts = Counter(y)\n",
    "        probabilities = np.array(list(class_counts.values())) / len(y)\n",
    "        return 1 - np.sum(probabilities ** 2)\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        \"\"\"Returns the most common class label in a node.\"\"\"\n",
    "        return Counter(y).most_common(1)[0][0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predicts class labels for given samples.\"\"\"\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        \"\"\"Traverses the tree to make a prediction.\"\"\"\n",
    "        if node.is_leaf():\n",
    "            return node.value\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        else:\n",
    "            return self._traverse_tree(x, node.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Decision Tree Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train our decision tree\n",
    "tree = DecisionTree(max_depth=5)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "# Check accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Custom Decision Tree Accuracy: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
